# src/backend/nlp/train_intent_model.py
"""
ì™œ LogisticRegression?
ê°„ë‹¨í•˜ê³  ë¹ ë¦„

ë¶„ë¥˜ ì‘ì—…(ì˜ë„ ë¶„ë¥˜)ì— ë§¤ìš° ìì£¼ ì“°ì´ëŠ” ê³ ì „ ëª¨ë¸

ë‚˜ì¤‘ì— KNN, SVM, XGBoost ë˜ëŠ” ë”¥ëŸ¬ë‹ìœ¼ë¡œë„ í™•ì¥ ê°€ëŠ¥

â“ ì¶”ê°€ ì§ˆë¬¸ ìƒê°í•´ë³¼ ë§Œí•œ ê²ƒ
â€œTF-IDF ë§ê³  ë‹¤ë¥¸ ë²¡í„°í™” ë°©ë²•ì€?â€ â†’ BERT/SBERT

â€œë¡œì§€ìŠ¤í‹± ë§ê³  ë‹¤ë¥¸ ëª¨ë¸ì€?â€ â†’ KNN, LSTM, transformer ë“±

â€œë°ì´í„°ê°€ ì ìœ¼ë©´?â€ â†’ Rule ê¸°ë°˜ + ìƒ˜í”Œ í™•ì¥

ğŸ§  ì •ë¦¬: ì§€ê¸ˆ ì´ê±¸ ì¶”ì²œë“œë ¤ìš”
ìš©ë„	ì¶”ì²œ ëª¨ë¸
Intent ë¶„ë¥˜	âœ… TF-IDF + LogisticRegression ë˜ëŠ” SBERT
Entity ì¶”ì¶œ	âœ… ì •ê·œì‹ â†’ spaCy/KoBERT (ì¶”í›„)
í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰	âœ… SBERT
ë¬¸ë§¥ + ì˜ë¯¸ ê³ ë ¤ ë¶„ë¥˜	âŒ ì§€ê¸ˆì€ BERT fine-tuningì€ ë¬´ë¦¬ (ë°ì´í„° ë¶€ì¡±)
ì¥ê¸°ì ìœ¼ë¡œ	âœ… HuggingFace KoBERT fine-tuning (NER)

ì§€ê¸ˆ ì œì¼ íš¨ìœ¨ì ì¸ ê±´?	TF-IDF + LogisticRegression + SBERT (ìœ ì‚¬ë„ ê²€ìƒ‰ìš©)
ë‚˜ì¤‘ì— ë°ì´í„° ë§ì•„ì§€ë©´?	ê·¸ë•Œ KoBERT, Transformer fine-tuning ê°€ëŠ¥

SBERT(Sentence-BERT)
ğŸ§ª SBERT ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜ íë¦„
ê° intentë‹¹ ëŒ€í‘œ ë¬¸ì¥ 5~10ê°œ ì¤€ë¹„ (ì˜ˆ: "ì„œìš¸ì— ì¶•ì œ ë­ ìˆì–´?")

ì‚¬ì „ ì„ë² ë”© ì €ì¥

ì‚¬ìš©ì ì…ë ¥ â†’ SBERT ì„ë² ë”©

Cosine ìœ ì‚¬ë„ ë¹„êµ â†’ ê°€ì¥ ìœ ì‚¬í•œ intent ì„ íƒ
"""


import json
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

with open("dataset/intent_dataset.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

texts = [item["text"] for item in dataset]
labels = [item["intent"] for item in dataset]

# 1. ë²¡í„°í™”
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# 2. ëª¨ë¸ í•™ìŠµ
model = LogisticRegression()
model.fit(X, labels)

# 3. ì €ì¥
with open("intent_model.pkl", "wb") as f:
    pickle.dump(model, f)

with open("tfidf_vectorizer.pkl", "wb") as f:
    pickle.dump(vectorizer, f)

print("âœ… Intent ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
