# src/backend/nlp/train_intent_model.py
"""
ëª©í‘œ: SBERT + HuggingFace KoBERT Fine-tuning

1ë‹¨ê³„: SBERTë¡œ ì¸í…íŠ¸ ì¶”ì¶œ (Baseline)
ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜: ë¬¸ì¥ ì„ë² ë”© â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ

í•™ìŠµ í•„ìš” ì—†ìŒ (ë°ì´í„° ì ì„ ë•Œ ë§¤ìš° íš¨ê³¼ì )

2ë‹¨ê³„: KoBERT Fine-tuningìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
ì¸í…íŠ¸ ë¶„ë¥˜ìš© supervised fine-tuning

ë°ì´í„° ë§ì•„ì§€ë©´ ë”¥ëŸ¬ë‹ ì „í™˜ (transformers ì‚¬ìš©)

1ë‹¨ê³„ - SBERT ìœ ì‚¬ë„ ê¸°ë°˜ â†’ ë² ì´ìŠ¤ë¼ì¸ í™•ë³´
2ë‹¨ê³„ - ë°ì´í„° ëŠ˜ì–´ë‚˜ë©´ KoBERT fine-tuning
3ë‹¨ê³„ - Entity ì¶”ì¶œ / Context ëª¨ë¸ ì¶”ê°€ë¡œ í™•ì¥

ëª©í‘œ: SBERT + KoBERT ê²°í•© ì¸í…íŠ¸ ë¶„ë¥˜
1ì°¨ ë¶„ë¥˜ëŠ” ë¹ ë¥¸ SBERT ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³ ,
**ì‹ ë¢°ë„(ìœ ì‚¬ë„ ì ìˆ˜)**ê°€ ë‚®ìœ¼ë©´ KoBERT fine-tuned ëª¨ë¸ë¡œ ë³´ì™„.

[ì…ë ¥ ë¬¸ì¥]
     â”‚
     â–¼
[SBERT ì„ë² ë”©]
     â”‚
     â–¼
[ê° intent ëŒ€í‘œë¬¸ì¥ê³¼ ìœ ì‚¬ë„ ê³„ì‚°]
     â”‚
     â–¼
[ìµœê³  ìœ ì‚¬ë„ + ìœ ì‚¬ë„ ì ìˆ˜]
     â”‚
     â”œâ”€â”€â”€ High confidence (ì˜ˆ: 0.75â†‘) â†’ SBERT ê²°ê³¼ ì±„íƒ
     â”‚
     â””â”€â”€â”€ Low confidence â†’ KoBERT ëª¨ë¸ë¡œ ì¬ë¶„ë¥˜
                          â”‚
                          â–¼
                     ìµœì¢… ì¸í…íŠ¸ ì¶œë ¥

"""
from sentence_transformers import SentenceTransformer, util
import json
import pickle
import torch

class SbertIntentClassifier:
    def __init__(self, sbert_model_path, intent_dataset_path, embedding_cache_path=None):
        self.model = SentenceTransformer(sbert_model_path)

        with open(intent_dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        self.intent_texts = {}
        for item in dataset:
            self.intent_texts.setdefault(item["intent"], []).append(item["text"])

        # ìºì‹œëœ ì„ë² ë”©ì´ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if embedding_cache_path and self._load_embeddings(embedding_cache_path):
            print("âœ… ìºì‹œëœ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")
        else:
            print("ğŸ”„ SBERT ì„ë² ë”© ìƒì„± ì¤‘...")
            self.intent_embeddings = {
                intent: self.model.encode(texts, convert_to_tensor=True)
                for intent, texts in self.intent_texts.items()
            }
            if embedding_cache_path:
                with open(embedding_cache_path, "wb") as f:
                    pickle.dump(self.intent_embeddings, f)
                print(f"âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ: {embedding_cache_path}")

    def _load_embeddings(self, path):
        try:
            with open(path, "rb") as f:
                self.intent_embeddings = pickle.load(f)
            return True
        except FileNotFoundError:
            return False

    def predict(self, text, threshold=0.65):
        input_embedding = self.model.encode(text, convert_to_tensor=True)

        best_intent, best_score = None, -1
        for intent, embeddings in self.intent_embeddings.items():
            score = util.cos_sim(input_embedding, embeddings).max().item()
            if score > best_score:
                best_intent, best_score = intent, score

        if best_score >= threshold:
            return best_intent, best_score
        else:
            return "unknown", best_score

clf = SbertIntentClassifier(
    sbert_model_path="snunlp/KR-SBERT-V40K-klueNLI-augSTS",
    intent_dataset_path="dataset/intent_dataset.json",
    embedding_cache_path="intent_embeddings.pkl"
)

intent, score = clf.predict("ìš”ì¦˜ ì„œìš¸ì— ë¬´ìŠ¨ í–‰ì‚¬ ìˆì–´?")
print(f"ì˜ˆì¸¡ ì¸í…íŠ¸: {intent} (ì ìˆ˜: {score:.4f})")
