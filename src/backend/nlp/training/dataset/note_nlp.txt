# 문장을 입력받으면 아래 포맷이 자동으로 만들어지게
# 순서
    ✔ Intent → ✔ Tokenizer → ✔ NER → ✔ Start/End 계산 → JSON 생성

1. Intent Dataset 준비 (intent_dataset.json)
2. Intent 분류 모델 만들기
3. Tokenizer (soynlp, mecab, HuggingFace tokenizer)
4. NER 모델 만들기(BIO 태깅 예측)
5. Start/ End 인덱스 자동 계산기 (python code)
6. 예상 json return
    {
    "text": "오늘 볼만한 공연 있어?",
    "intent": "공연",
    "tokens": [
        { "word": "오늘", "tag": "B-DATE", "start": 0, "end": 2 },
        { "word": "볼만한", "tag": "O", "start": 3, "end": 6 },
        { "word": "공연", "tag": "B-EVENT", "start": 7, "end": 9 },
        { "word": "있어", "tag": "O", "start": 10, "end": 12 },
        { "word": "?", "tag": "O", "start": 12, "end": 13 }
    ]
    }

Todolist
1. intent_dataset.json 을참고해서 다양한 주제의 intent dataset 작성 (정원민)
2. intent dataset을 가지고 Intent 분류모델만들기 (->intent return) (소형섭)
3. Tokenizer로 intent_dataset.json의 txt를 단어 단위로 변환 (소형섭)
4. NER 모델 만들기(BIO 태깅 예측) (소형섭)
5. 각 단어의 Start/ End 인덱스 자동 계산기(python code) (소형섭)

예시 통합 함수
def process(text):
    intent = predict_intent(text)                 # 2단계
    tokens = tokenize(text)                       # 3단계
    tags = predict_ner_tags(tokens)               # 4단계
    token_data = add_start_end(text, tokens, tags) # 5단계
    return {
        "text": text,
        "intent": intent,
        "tokens": token_data
    }
    
제한사항
    정원민-intent_dataset.json location: src/backend/nlp/training/dataset/
    소형섭-train_intent_model.py location: src/backend/nlp/training/
    소형섭-tokenizer.py location : src/backend/nlp/training/
    소형섭-train_ner_model.py location : src/backend/nlp/training/
    소형섭-index_mapper.py location: src/backend/nlp/training/
    소형섭-nlu_final.py location: src/backend/nlp/training/
